{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec41881e-bc8e-4050-9b11-53c931c2529b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.36.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (1.8.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (1.5.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (10.3.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (15.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (3.1.43)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from streamlit) (6.4)\n",
      "Requirement already satisfied: jinja2 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.22.0)\n",
      "Requirement already satisfied: toolz in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Using cached streamlit-1.36.0-py2.py3-none-any.whl (8.6 MB)\n",
      "Using cached altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pydeck, altair, streamlit\n",
      "Successfully installed altair-5.3.0 pydeck-0.9.1 streamlit-1.36.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0437a29-2679-49ef-bf06-7610b9e94fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import os\n",
    "from datetime import datetime\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "import io\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59799987-e873-4106-ba92-1ad374ea9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_pdfs_on_github(query, num_urls=5):\n",
    "    load_dotenv()\n",
    "    base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    #GOOGLE_SEARCH_API\n",
    "    api_key = os.environ.get(\"GOOGLE_SEARCH_API\")\n",
    "    # GOOGLE_SEARCH_ENGINE_ID\n",
    "    search_engine_id = os.environ.get(\"GOOGLE_SEARCH_ENGINE_ID\")\n",
    "    \n",
    "    params = {\n",
    "        \"q\": f\"{query} site:github.com filetype:pdf\",\n",
    "        \"key\": api_key,\n",
    "        \"cx\": search_engine_id,\n",
    "        \"num\": num_urls\n",
    "    }\n",
    "    \n",
    "    print(f\"Request parameters: {params}\")\n",
    "    print(f\"Request URL: {base_url}?{urllib.parse.urlencode(params)}\")\n",
    "    \n",
    "    print(\"Sending request to Google Custom Search API...\")\n",
    "    response = requests.get(base_url, params=params)\n",
    "    print(\"Response received.\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Parsing response data...\")\n",
    "        data = response.json()\n",
    "        print(\"Response data parsed.\")\n",
    "        print(f\"Response data: {data}\")\n",
    "        \n",
    "        pdf_urls = []\n",
    "        if \"items\" in data:\n",
    "            print(\"Found 'items' in response data.\")\n",
    "            for item in data[\"items\"]:\n",
    "                pdf_url = item[\"link\"]\n",
    "                pdf_urls.append(pdf_url)\n",
    "                print(f\"Added URL: {pdf_url}\")\n",
    "        else:\n",
    "            print(\"No 'items' found in response data.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.json())\n",
    "        pdf_urls = []\n",
    "    \n",
    "    print(f\"Found {len(pdf_urls)} PDF URLs.\")\n",
    "    return pdf_urls\n",
    "\n",
    "\n",
    "def create_folder(name, parent_id=None):\n",
    "    file_metadata = {\n",
    "        'name': name,\n",
    "        'mimeType': 'application/vnd.google-apps.folder'\n",
    "    }\n",
    "    if parent_id:\n",
    "        file_metadata['parents'] = [parent_id]\n",
    "    folder = drive_service.files().create(body=file_metadata, fields='id').execute()\n",
    "    return folder.get('id')\n",
    "\n",
    "def find_or_create_folder(name, parent_id=None):\n",
    "    query = f\"name='{name}' and mimeType='application/vnd.google-apps.folder'\"\n",
    "    if parent_id:\n",
    "        query += f\" and '{parent_id}' in parents\"\n",
    "    results = drive_service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()\n",
    "    folders = results.get('files', [])\n",
    "    if folders:\n",
    "        return folders[0]['id']\n",
    "    else:\n",
    "        return create_folder(name, parent_id)\n",
    "\n",
    "\n",
    "def download_and_upload_pdf(drive_service, url, folder_id):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        file_name = os.path.basename(url)\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        file_metadata = {'name': file_name, 'parents': [folder_id]}\n",
    "        media = MediaFileUpload(file_name, resumable=True)\n",
    "        file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "        \n",
    "        os.remove(file_name)  # Remove the local file after uploading\n",
    "        print(f\"File {file_name} uploaded successfully.\")\n",
    "        return file.get('id')\n",
    "    else:\n",
    "        print(f\"Failed to download {url}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def download_file(drive_service, file_id, file_name):\n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    fh = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "    fh.seek(0)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(fh.read())\n",
    "\n",
    "def pdf_to_text(file_path):\n",
    "    elements = partition_pdf(filename=file_path)\n",
    "    return '\\n'.join([str(element) for element in elements])\n",
    "\n",
    "def process_pdf(drive_service, file_id, file_name, folder_id):\n",
    "    print(f\"Processing {file_name}...\")\n",
    "\n",
    "    # Download the file\n",
    "    download_file(drive_service, file_id, file_name)\n",
    "\n",
    "    # Convert PDF to text\n",
    "    text_content = pdf_to_text(file_name)\n",
    "\n",
    "    # Save text content to a file\n",
    "    text_file_name = f\"{os.path.splitext(file_name)[0]}.txt\"\n",
    "    with open(text_file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write(text_content)\n",
    "\n",
    "    # Upload text file to Google Drive\n",
    "    file_metadata = {'name': text_file_name, 'parents': [folder_id]}\n",
    "    media = MediaFileUpload(text_file_name, resumable=True)\n",
    "    drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "\n",
    "    # Clean up local files\n",
    "    os.remove(file_name)\n",
    "    os.remove(text_file_name)\n",
    "\n",
    "    print(f\"Processed {file_name} and uploaded {text_file_name}\")\n",
    "\n",
    "def upload_pdf_to_google_drive(drive_service, pdf_urls, subfolder_id, num_files_to_process=None):\n",
    "    # Download and upload PDFs\n",
    "    uploaded_files = []\n",
    "    for url in pdf_urls[:num_files_to_process]:\n",
    "        file_id = download_and_upload_pdf(drive_service, url, subfolder_id)\n",
    "        if file_id:\n",
    "            uploaded_files.append((file_id, os.path.basename(url)))\n",
    "\n",
    "    print(\"All PDFs have been uploaded to Google Drive.\")\n",
    "\n",
    "    # Process uploaded PDFs\n",
    "    print(\"Processing PDFs...\")\n",
    "    for file_id, file_name in uploaded_files:\n",
    "        process_pdf(drive_service, file_id, file_name, subfolder_id)\n",
    "\n",
    "    print(\"All PDFs have been processed and converted to text.\")\n",
    "\n",
    "\n",
    "def download_file(drive_service, file_id, file_name):\n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    fh = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "    fh.seek(0)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(fh.read())\n",
    "\n",
    "def pdf_to_text(file_path):\n",
    "    try:\n",
    "        elements = partition_pdf(filename=file_path)\n",
    "        return '\\n'.join([str(element) for element in elements])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_pdfs(drive_service, folder_id, local_dir, num_files=None):\n",
    "    # Create local directory if it doesn't exist\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    \n",
    "    # List files in the Google Drive folder\n",
    "    query = f\"'{folder_id}' in parents and mimeType='application/pdf'\"\n",
    "    results = drive_service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()\n",
    "    files = results.get('files', [])\n",
    "    \n",
    "    # Limit the number of files to process if specified\n",
    "    if num_files is not None:\n",
    "        files = files[:num_files]\n",
    "    \n",
    "    for file in files:\n",
    "        file_name = file['name']\n",
    "        file_id = file['id']\n",
    "        local_path = os.path.join(local_dir, file_name)\n",
    "        \n",
    "        print(f\"Downloading {file_name}...\")\n",
    "        download_file(drive_service, file_id, local_path)\n",
    "        \n",
    "        print(f\"Converting {file_name} to text...\")\n",
    "        text_content = pdf_to_text(local_path)\n",
    "        \n",
    "        if text_content:\n",
    "            # Save text content\n",
    "            text_file_path = os.path.splitext(local_path)[0] + '.txt'\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text_content)\n",
    "            print(f\"Processed {file_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to process {file_name}\")\n",
    "    \n",
    "    print(\"All specified PDFs have been downloaded and converted to text.\")\n",
    "\n",
    "\n",
    "def process_last_text_file(directory=\"pdfs_to_convert_to_text\", preview_length=100):\n",
    "    # Get all text files in the directory\n",
    "    txt_files = glob.glob(os.path.join(directory, \"*.txt\"))\n",
    "    \n",
    "    # Sort the files to ensure consistent ordering\n",
    "    txt_files.sort()\n",
    "    \n",
    "    created_variable_name = None\n",
    "    content = \"\"\n",
    "    \n",
    "    if txt_files:\n",
    "        # Get the last text file\n",
    "        last_txt_file = txt_files[-1]\n",
    "        \n",
    "        # Extract the base name and create the variable name\n",
    "        base_name = os.path.basename(last_txt_file)\n",
    "        file_name_without_ext = os.path.splitext(base_name)[0]\n",
    "        variable_name = f\"{file_name_without_ext}_str\"\n",
    "        \n",
    "        # Read the content of the last file\n",
    "        with open(last_txt_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Create a variable with the file name and _str suffix\n",
    "        globals()[variable_name] = content\n",
    "        created_variable_name = variable_name\n",
    "        \n",
    "        print(f\"Created variable: {variable_name}\")\n",
    "        print(f\"Content preview: {content[:preview_length]}...\")  # Print preview_length characters as preview\n",
    "    else:\n",
    "        print(\"No text files found in the directory.\")\n",
    "    \n",
    "    # Now, let's use the created variable\n",
    "    if created_variable_name:\n",
    "        # Print every third character of the preview\n",
    "        print(f\"Every third character of the preview from {created_variable_name}:\")\n",
    "        print(content[:preview_length][::3])\n",
    "    else:\n",
    "        print(\"No variable was created because no text files were found.\")\n",
    "    \n",
    "    return created_variable_name, content\n",
    "\n",
    "\n",
    "def chunk_text(text, max_chunk_size=5000):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    for word in words:\n",
    "        if current_size + len(word) + 1 > max_chunk_size:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_size = len(word)\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_size += len(word) + 1\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def text_to_speech(text, file_name, output_directory=\"audio_voiceovers\", chunk_index=None):\n",
    "    load_dotenv()\n",
    "    CHUNK_SIZE = 1024\n",
    "    XI_API_KEY = os.environ.get(\"XI_API_KEY\")\n",
    "    VOICE_ID = os.environ.get(\"VOICE_ID\")\n",
    "    tts_url = f\"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream\"\n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": XI_API_KEY\n",
    "    }\n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"model_id\": \"eleven_multilingual_v2\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.8,\n",
    "            \"style\": 0.0,\n",
    "            \"use_speaker_boost\": True\n",
    "        }\n",
    "    }\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if chunk_index is not None:\n",
    "        output_file_name = f\"{timestamp}_{file_name}_part{chunk_index+1}_audio.mp3\"\n",
    "    else:\n",
    "        output_file_name = f\"{timestamp}_{file_name}_audio.mp3\"\n",
    "    \n",
    "    output_path = os.path.join(output_directory, output_file_name)\n",
    "    response = requests.post(tts_url, json=data, headers=headers, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"Audio stream saved successfully to {output_path}\")\n",
    "        return output_path\n",
    "    else:\n",
    "        print(f\"Error in text-to-speech conversion: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "def get_text_files(directory):\n",
    "    return [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def process_text_file(file_path, output_directory, delay_between_chunks):\n",
    "    content = read_text_file(file_path)\n",
    "    file_name_without_ext = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    print(f\"Converting {os.path.basename(file_path)} to speech...\")\n",
    "    \n",
    "    text_chunks = chunk_text(content)\n",
    "    \n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        print(f\"Processing chunk {i+1} of {len(text_chunks)}...\")\n",
    "        audio_file_path = text_to_speech(chunk, file_name_without_ext, output_directory, i)\n",
    "        \n",
    "        if audio_file_path:\n",
    "            print(f\"Audio file saved as: {audio_file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to convert chunk {i+1} of {file_path} to speech.\")\n",
    "        \n",
    "        if i < len(text_chunks) - 1:\n",
    "            print(f\"Waiting for {delay_between_chunks} seconds before processing the next chunk...\")\n",
    "            time.sleep(delay_between_chunks)\n",
    "\n",
    "# def convert_txt_to_audio_voiceover(text_directory = \"pdfs_to_convert_to_text\", output_directory = \"audio_voiceovers\"):\n",
    "#     delay_between_chunks = 60  # Adjust this value as needed\n",
    "\n",
    "#     text_files = get_text_files(text_directory)\n",
    "    \n",
    "#     for text_file in text_files:\n",
    "#         file_path = os.path.join(text_directory, text_file)\n",
    "#         process_text_file(file_path, output_directory, delay_between_chunks)\n",
    "    \n",
    "#     print(\"All text files have been processed.\")\n",
    "\n",
    "def convert_txt_to_audio_voiceover(text_directory=\"pdfs_to_convert_to_text\", output_directory=\"audio_voiceovers\"):\n",
    "    delay_between_chunks = 60  # Adjust this value as needed\n",
    "    text_files = get_text_files(text_directory)\n",
    "    \n",
    "    mp3_paths = []\n",
    "    for text_file in text_files:\n",
    "        file_path = os.path.join(text_directory, text_file)\n",
    "        mp3_path = process_text_file(file_path, output_directory, delay_between_chunks)\n",
    "        mp3_paths.append(mp3_path)\n",
    "    \n",
    "    print(\"All text files have been processed.\")\n",
    "    \n",
    "    # Return the path of the last generated MP3 file\n",
    "    return mp3_paths[-1] if mp3_paths else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "955ffbcd-a0fc-4fb7-b248-f23e69c39da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for PDF URLs...\n",
      "Request parameters: {'q': 'observability site:github.com filetype:pdf', 'key': 'AIzaSyDh6OU3OFcn_t3vrxO2fzGU49cT6pXpW0g', 'cx': 'f4afc285f53804afb', 'num': 5}\n",
      "Request URL: https://www.googleapis.com/customsearch/v1?q=observability+site%3Agithub.com+filetype%3Apdf&key=AIzaSyDh6OU3OFcn_t3vrxO2fzGU49cT6pXpW0g&cx=f4afc285f53804afb&num=5\n",
      "Sending request to Google Custom Search API...\n",
      "Response received.\n",
      "Parsing response data...\n",
      "Response data parsed.\n",
      "Response data: {'kind': 'customsearch#search', 'url': {'type': 'application/json', 'template': 'https://www.googleapis.com/customsearch/v1?q={searchTerms}&num={count?}&start={startIndex?}&lr={language?}&safe={safe?}&cx={cx?}&sort={sort?}&filter={filter?}&gl={gl?}&cr={cr?}&googlehost={googleHost?}&c2coff={disableCnTwTranslation?}&hq={hq?}&hl={hl?}&siteSearch={siteSearch?}&siteSearchFilter={siteSearchFilter?}&exactTerms={exactTerms?}&excludeTerms={excludeTerms?}&linkSite={linkSite?}&orTerms={orTerms?}&dateRestrict={dateRestrict?}&lowRange={lowRange?}&highRange={highRange?}&searchType={searchType}&fileType={fileType?}&rights={rights?}&imgSize={imgSize?}&imgType={imgType?}&imgColorType={imgColorType?}&imgDominantColor={imgDominantColor?}&alt=json'}, 'queries': {'request': [{'title': 'Google Custom Search - observability site:github.com filetype:pdf', 'totalResults': '12', 'searchTerms': 'observability site:github.com filetype:pdf', 'count': 5, 'startIndex': 1, 'inputEncoding': 'utf8', 'outputEncoding': 'utf8', 'safe': 'off', 'cx': 'f4afc285f53804afb'}], 'nextPage': [{'title': 'Google Custom Search - observability site:github.com filetype:pdf', 'totalResults': '12', 'searchTerms': 'observability site:github.com filetype:pdf', 'count': 5, 'startIndex': 6, 'inputEncoding': 'utf8', 'outputEncoding': 'utf8', 'safe': 'off', 'cx': 'f4afc285f53804afb'}]}, 'context': {'title': 'github.com'}, 'searchInformation': {'searchTime': 0.228074, 'formattedSearchTime': '0.23', 'totalResults': '12', 'formattedTotalResults': '12'}, 'items': [{'kind': 'customsearch#result', 'title': 'Getting Started with Ingesting GitHub GHAS Alerts', 'htmlTitle': 'Getting Started with Ingesting GitHub GHAS Alerts', 'link': 'https://partner.github.com/assets/pdfs/Getting.Started.with.Ingesting.GitHub.GHAS.Alerts.pdf', 'displayLink': 'partner.github.com', 'snippet': '○ Observability. ○ Security Information and Event Management (SIEM). ○ Business Intelligence (BI). It contains links to documentation and sample code. The.', 'htmlSnippet': '○ <b>Observability</b>. ○ Security Information and Event Management (SIEM). ○ Business Intelligence (BI). It contains links to documentation and sample code. The.', 'formattedUrl': 'https://partner.github.com/.../Getting.Started.with.Ingesting.GitHub.GHAS....', 'htmlFormattedUrl': 'https://partner.github.com/.../Getting.Started.with.Ingesting.GitHub.GHAS....', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRgKrH4m3qOC3b0fuKb-MvJDkx_cvr4euPUJdNWZla8TDH8uylO3Horhr4&s', 'width': '257', 'height': '196'}], 'metatags': [{'creator': 'Google', 'title': 'Getting Started with Ingesting GitHub GHAS Alerts'}], 'cse_image': [{'src': 'x-raw-image:///0a9d9878f74650d8d72e88f0d78b4e70f6850a924d354e2dba7f2680e1897727'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': 'Thermal & Reliability Study on High Current Thermal Vias & Output ...', 'htmlTitle': 'Thermal &amp; Reliability Study on High Current Thermal Vias &amp; Output ...', 'link': 'https://github.com/rusefi/rusefi/wiki/PDFs/Synqor_thermal_relief_study.pdf', 'displayLink': 'github.com', 'snippet': 'This was done so that all current would flow through the observable spokes on the oppo- site side. The copper is specified to be one ounce, plated to two\\xa0...', 'htmlSnippet': 'This was done so that all current would flow through the <b>observable</b> spokes on the oppo- site side. The copper is specified to be one ounce, plated to two&nbsp;...', 'formattedUrl': 'https://github.com/rusefi/rusefi/wiki/.../Synqor_thermal_relief_study.pdf', 'htmlFormattedUrl': 'https://github.com/rusefi/rusefi/wiki/.../Synqor_thermal_relief_study.pdf', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRiKUFH18X6EMOQyl4SgFD7W_8vG_AKkPxjt26Ub9MlRLkAsuBVDCIJw6aj&s', 'width': '224', 'height': '224'}], 'metatags': [{'moddate': \"D:20040504132711-04'00'\", 'creationdate': 'D:20040504132710', 'creator': 'PScript5.dll Version 5.2', 'author': 'erico', 'producer': 'Acrobat Distiller 4.05 for Windows', 'title': 'an_Thermal_Relief_Study.qxd'}], 'cse_image': [{'src': 'x-raw-image:///1945082d73729850ce8e41c620cb13725ce1df8c25da98f09cdb849dce574c84'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': 'Spectral AMGe ( AMGe)', 'htmlTitle': 'Spectral AMGe ( AMGe)', 'link': 'https://github.com/hypre-space/hypre/wiki/pubs/UCRL_JC146369.pdf', 'displayLink': 'github.com', 'snippet': 'This reduction in operator complexity is observable (though relatively slight) in. Table 4.1; however, we note without demonstration that it is somewhat more\\xa0...', 'htmlSnippet': 'This reduction in operator complexity is <b>observable</b> (though relatively slight) in. Table 4.1; however, we note without demonstration that it is somewhat more&nbsp;...', 'formattedUrl': 'https://github.com/hypre-space/hypre/wiki/pubs/UCRL_JC146369.pdf', 'htmlFormattedUrl': 'https://github.com/hypre-space/hypre/wiki/pubs/UCRL_JC146369.pdf', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSblaiULUCzkpM5IU5h_hlx1jQYfS5sWuktbReDuWKze16yAvO3Ixcg7vWK&s', 'width': '224', 'height': '225'}], 'metatags': [{'moddate': \"D:20030227150238-08'00'\", 'creationdate': 'D:20030227225630Z', 'creator': 'dvips(k) 5.86 Copyright 1999 Radical Eye Software', 'producer': 'Acrobat Distiller 5.0 (Windows)', 'title': 'ms39892.dvi'}], 'cse_image': [{'src': 'x-raw-image:///4c2c5603a4372fc3d7051043921836e45d1b66d6fe28d009ac540b709edba663'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': 'GSoC 2021 Proposal - Write Frontend Tests (Praneeeth)', 'htmlTitle': 'GSoC 2021 Proposal - Write Frontend Tests (Praneeeth)', 'link': 'https://github.com/oppia/oppia/wiki/pdfs/GSoC2021GangavarapuPraneeth.pdf', 'displayLink': 'github.com', 'snippet': '○ This involves testing an Observable. This can be tested by mocking the eventemitter. This will run the code present inside the .subscribe. ○ Example\\xa0...', 'htmlSnippet': '○ This involves testing an <b>Observable</b>. This can be tested by mocking the eventemitter. This will run the code present inside the .subscribe. ○ Example&nbsp;...', 'formattedUrl': 'https://github.com/oppia/oppia/wiki/.../GSoC2021GangavarapuPraneeth.pd...', 'htmlFormattedUrl': 'https://github.com/oppia/oppia/wiki/.../GSoC2021GangavarapuPraneeth.pd...', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR_u8RH-ZwM4dRvqeCmwkEgolSp1DdPzowq6OnpiPZyEApzjQKKWUyMNnI&s', 'width': '311', 'height': '162'}], 'metatags': [{'producer': 'Skia/PDF m92 Google Docs Renderer', 'title': 'GSoC 2021 Proposal - Write Frontend Tests (Praneeeth)'}], 'cse_image': [{'src': 'x-raw-image:///7da91f606dd0310bf13969b0e14a868f8705050524a94c3cb6c2eb0295512c35'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}, {'kind': 'customsearch#result', 'title': 'Dependently Typed Metaprogramming (in Agda)', 'htmlTitle': 'Dependently Typed Metaprogramming (in Agda)', 'link': 'https://raw.github.com/pigworker/MetaprogAgda/master/notes.pdf', 'displayLink': 'raw.github.com', 'snippet': 'Aug 26, 2013 ... ... observable proper- ties of values and is thus coherent in the sense that coh : (X Y : TU) (Q : [ X ↔ Y ]TU) (x : [ X ]TU) → [ Eq X x Y\\xa0...', 'htmlSnippet': 'Aug 26, 2013 <b>...</b> ... <b>observable</b> proper- ties of values and is thus coherent in the sense that coh : (X Y : TU) (Q : [ X ↔ Y ]TU) (x : [ X ]TU) → [ Eq X x Y&nbsp;...', 'formattedUrl': 'https://raw.github.com/pigworker/MetaprogAgda/master/notes.pdf', 'htmlFormattedUrl': 'https://raw.github.com/pigworker/MetaprogAgda/master/notes.pdf', 'pagemap': {'metatags': [{'moddate': \"D:20130826172014+01'00'\", 'creator': 'TeX', 'creationdate': \"D:20130826172014+01'00'\", 'fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'producer': 'pdfTeX-1.40.12'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}]}\n",
      "Found 'items' in response data.\n",
      "Added URL: https://partner.github.com/assets/pdfs/Getting.Started.with.Ingesting.GitHub.GHAS.Alerts.pdf\n",
      "Added URL: https://github.com/rusefi/rusefi/wiki/PDFs/Synqor_thermal_relief_study.pdf\n",
      "Added URL: https://github.com/hypre-space/hypre/wiki/pubs/UCRL_JC146369.pdf\n",
      "Added URL: https://github.com/oppia/oppia/wiki/pdfs/GSoC2021GangavarapuPraneeth.pdf\n",
      "Added URL: https://raw.github.com/pigworker/MetaprogAgda/master/notes.pdf\n",
      "Found 5 PDF URLs.\n",
      "Search completed.\n",
      "PDF URLs found:\n",
      "https://partner.github.com/assets/pdfs/Getting.Started.with.Ingesting.GitHub.GHAS.Alerts.pdf\n",
      "https://github.com/rusefi/rusefi/wiki/PDFs/Synqor_thermal_relief_study.pdf\n",
      "https://github.com/hypre-space/hypre/wiki/pubs/UCRL_JC146369.pdf\n",
      "https://github.com/oppia/oppia/wiki/pdfs/GSoC2021GangavarapuPraneeth.pdf\n",
      "https://raw.github.com/pigworker/MetaprogAgda/master/notes.pdf\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_urls = 5\n",
    "keywords = \"observability\"\n",
    "query = keywords\n",
    "\n",
    "print(\"Searching for PDF URLs...\")\n",
    "pdf_urls = search_pdfs_on_github(query, num_urls)\n",
    "print(\"Search completed.\")\n",
    "\n",
    "print(\"PDF URLs found:\")\n",
    "for url in pdf_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3da97d3-008e-46f0-8b4e-9914c7db58e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=131811907665-252blpd0att90p8qbeiifjf4cpsh20e4.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8081%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.file&state=kTEgf2Z9dduIZXRxbuXNVVbTMy4x4D&access_type=offline\n",
      "File Getting.Started.with.Ingesting.GitHub.GHAS.Alerts.pdf uploaded successfully.\n",
      "File Synqor_thermal_relief_study.pdf uploaded successfully.\n",
      "All PDFs have been uploaded to Google Drive.\n",
      "Processing PDFs...\n",
      "Processing Getting.Started.with.Ingesting.GitHub.GHAS.Alerts.pdf...\n",
      "Processed Getting.Started.with.Ingesting.GitHub.GHAS.Alerts.pdf and uploaded Getting.Started.with.Ingesting.GitHub.GHAS.Alerts.txt\n",
      "Processing Synqor_thermal_relief_study.pdf...\n",
      "Processed Synqor_thermal_relief_study.pdf and uploaded Synqor_thermal_relief_study.txt\n",
      "All PDFs have been processed and converted to text.\n"
     ]
    }
   ],
   "source": [
    "# Define the scopes you need\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "# Create the flow using the client secrets file from the Google API Console.\n",
    "flow = InstalledAppFlow.from_client_secrets_file(\n",
    "    'credentials.json',\n",
    "    scopes=SCOPES,\n",
    "    redirect_uri='http://localhost:8081/'  # Ensure this matches the configured redirect URI\n",
    ")\n",
    "\n",
    "# Run the OAuth flow\n",
    "creds = flow.run_local_server(port=8081)\n",
    "\n",
    "# Build the Drive service\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Create main folder 'getpdfs' if it doesn't exist\n",
    "main_folder_id = find_or_create_folder('getpdfs')\n",
    "\n",
    "# Create subfolder with today's date and time\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "subfolder_id = create_folder(current_time, main_folder_id)\n",
    "\n",
    "upload_pdf_to_google_drive(drive_service, pdf_urls, subfolder_id, num_files_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "495ce1e9-bfea-4c71-950c-7f4089ffe10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many files do you want to process? (Enter a number or press Enter for all):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Synqor_thermal_relief_study.pdf...\n",
      "Converting Synqor_thermal_relief_study.pdf to text...\n",
      "Processed Synqor_thermal_relief_study.pdf\n",
      "All specified PDFs have been downloaded and converted to text.\n"
     ]
    }
   ],
   "source": [
    "local_directory = \"pdfs_to_convert_to_text\"\n",
    "num_files_to_process = 1\n",
    "\n",
    "process_pdfs(drive_service, subfolder_id, local_directory, num_files_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52f6dacd-b7f4-49bf-ae0d-c735b624eac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created variable: Synqor_thermal_relief_study_str\n",
      "Content preview: Thermal & Reliability Study on High Current Thermal Vias & Output Pins Application Note 00-08-01 Rev...\n",
      "Every third character of the preview from Synqor_thermal_relief_study_str:\n",
      "Trl lbi u  gCrthm a&uuPspitno -- v\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thermal & Reliability Study on High Current Thermal Vias & Output Pins Application Note 00-08-01 Rev. 03 - 7/31/01\\nSummary:\\nThis application note addresses concerns raised with regards to pin and board heating when high currents (>60A) are driven through ther- mally relieved plated through hole (PTH) vias. Detailed thermal analy- sis will show that internal heat generation and resulting temperature rise of the power converter and load board is minimal when using a single thermally relieved via.\\nIntroduction Trends in the design of Distributed Power Architectures (DPA\\'s) are requiring DC-DC converters to deliver very high power at very low voltages. This has required a thorough examination on how high current power can be delivered safely and reliably to the distribution path within a printed circuit card. SynQor has evaluated these challenges through theoretical analysis and carefully controlled laboratory testing. Using the latest in thermal imaging equipment as well as analytical calculations, SynQor has concluded that currents in excess of 60A can be safely and reliably delivered through a single pin. In addition, SynQor demonstrates that reasonable thermal relief geometries do not have significant self heating at 60A.\\n1.0 Pin Resistance The first concern is that the resistive loss in a single output pin might contribute excessively to power losses in the distribution path or create a severe temperature rise within the pin due to self heating from I-R losses. It is rel- atively simple to calculate the resistance of a standard output pin. The measured resistance of SynQor\\'s stan- dard 0.080\" brass output pin is 0.14mΩ. Using Equation 1 the total power dissipated within a pin delivering 60A is 0.504W. From these equations we can see that the power dissipation is relatively small when com- pared to the total power dissipation in the unit when delivering 60A. The efficiency and power dissipation spec- ified in SynQor\\'s data sheets includes the effect of pin resistance. The pin temperature increase, albeit small, is factored into the power derating curves on the data sheet, since the thermal connection between the pin and the SynQor PC board is very good. It should also be noted that the total voltage drop across both output pins is about 16mV, which is likely much smaller than the further copper distribution losses.\\nEquation 1:\\nPdiss = I2R\\n2.0 Multiple Output Pins and Distribution Losses Another common assumption is that by having multiple output pins, power losses will be reduced in the distri- bution path due to an overall decrease in the distribution path resistance. This is true for the circuit as drawn in Figure 1, where R1 and R2 represent the distribution losses in a circuit card, and distance D1 is similar to dis- tance D2. In this situation the total losses are calculated in Equation 2. Table 1 shows that two pins with half the current spaced far enough apart can have a much lower power loss than 1 pin carrying the same current.\\nEquation 2:\\nTotal Power Lost Figure 1 = i1\\n2*R1 + i2\\n2*R2\\nSynQor - Advancing the Power Curve • 888-567-9596 (cid:127) www.synqor.com\\nPage 1\\nApplication Note 00-08-01\\nHigh Current Thermal Vias & Output Pins\\nDistance D2\\nDistance D2\\nPin 1\\ni1\\nR1\\n1 D e c n a t s i D\\nPin 1\\nPin 2\\ni1\\nR1\\n1 D e c n a t s i D\\ni2\\nR2\\nRload\\nRload\\nPin 2\\nFigure 1: Converter layout with 2 output pins\\nFigure 2: Converter layout with 2 output pins\\nspaced far apart (non-practical)\\nspaced close together (typical)\\nFor a typical circuit card and converter however, the distance D1 is much smaller than D2 as shown in Figure 2, and hence the overall power loss is given by equation 3. Two output pins placed closely together offers no advantage if the distance between the pins is small compared to the size of the distribution path, as is the case for all commercially available converters.\\nEquation 3:\\nTotal Power Lost Figure 2 = i1\\n2*R1\\nCircuit\\nI1\\nR1\\nI2\\nR2\\nPdiss\\nFigure 1\\n30A\\n0.5mΩ\\n30A\\n0.7mΩ\\n1.10W\\nFigure 2\\n60A\\n0.5mΩ\\n0\\n0\\n1.80W\\nTable 1: Dissipated Power of circuits from Figures 1 & 2.\\n3.0 Thermal Relief Designs and Power Loss - Predicted Another challenge facing today\\'s power designer is the use of thermal reliefs in the design of high current Printed Circuit Boards (PCB\\'s). Standards in the design of high reliability circuit packs require that design- ers use thermal reliefs to insure reliable solder connections between converter pins and the circuit card dur- ing the soldering process. The use of these thermal reliefs are often viewed as highly resistive elements that can contribute to significant self heating within a circuit card, especially if only one pin is available to deliv- er current. Through calculation and detailed analysis of thermal images taken in SynQor\\'s laboratory, SynQor demonstrates that a properly designed thermal relief does not create a reliability concern due to self heating within the thermal relief. The data clearly shows that while there is a thermal gradient from the hot- ter converter board to the cooler customer board, there is very little heating due to the resistance of the spokes in the thermal relief.\\nTheoretical Analysis\\nIn order to find the heat generated in a conductor when current passes through it, the resistance of the con- ductor must be known. The resistance is defined by the Equation 4.\\nSynQor - Advancing the Power Curve (cid:127) 888-567-9596 (cid:127) www.synqor.com\\nPage 2\\nApplication Note 00-08-01\\nHigh Current Thermal Vias & Output Pins\\nEquation 4\\nR = length/(σr*width*thickness) where [σr] is 50 E6 amps2/Watt⋅meter for copper, and the geometry pertains to a conductor with a rectangular cross-section.\\nThe heat generated is [i2R], so the heat generation per unit volume [Qdotv\\'\\'\\'] of the conductor is [i2R/(area*length)]. For a conductor with a rectangular cross section, [A=w*t], so:\\nEquation 5\\nQdotv\\'\\'\\' = i2/(σ\\nr*w2*t2)\\nFor a small length [∆x] of the conductor, the rate of heat transfer into the system boundary minus the heat trans- fer out of the system boundary plus the heat generated within the system must equal zero.\\nEquation 6\\nQdot(x)-Qdot(x+∆x)+i2/(σ\\nr*w2*t2)*∆x*t*w = 0\\nDividing both sides by [∆x] and allowing [∆x] to approach zero yields:\\nEquation 7\\ndQdot/dx = i2/(σ\\nr*w*t)\\nSubstituting the heat flux times the area for [Qdot] (Fourier\\'s Law), and dividing both sides by [w*t*k],\\nEquation 8\\nd/dx[dT/dx] = i2/(σ\\nr*w2*t2)/(-k)\\nwhere the thermal conductivity [k] is 386 Watts/meter°C for copper.\\nIntegrating once reveals\\nEquation 9\\ndT/dx = i2/(σ\\nr*w2*t2)/(-k)*x + C1\\nA second integration gives us the following:\\nEquation 10\\nT(x) = i2/(σ\\nr*w2*t2)*x2/(2*(-k)) + C1*x + C2.\\nOur boundary conditions are that the temperature at [x=0] is the pin temperature [Tpin] and that the temperature at [x=l] is the load board temperature [Tboard]. When [x=0], [T(x)] = [C2], so [C2] is equal to [Tpin]. Setting [T(l)] equal to [Tboard] gives the following solution for [C1].\\nEquation 11 C1 = ( Tboard - Tpin - (i2*l2)/(2*t2*w2*σ\\nr*(-k)) )/ l\\nCombining Equations 10 and 11 gives the generalized solution for the temperature as a function of the position along the spoke length.\\nEquation 12\\nT(x) = i2*x2/(2*σ\\nr*w2*t2*(-k)) + (Tboard - Tpin - (i2*l2)/(2*t2*w2*σ\\nr*(-k)) )*x / l + Tpin\\nIf Equation 12 is rearranged, the temperature along the length of the spoke becomes the superposition of a lin- ear change in temperature between the pin and board plus a length dependent increase in temperature due to the i2R heating.\\nEquation 13\\nT(x) = Tboard*x/l + Tpin*(1-x/l) + i2/(2*σ\\nr*w2*t2*(-k))*(x2-l*x)\\nSynQor - Advancing the Power Curve (cid:127) 888-567-9596 (cid:127) www.synqor.com\\nPage 3\\nApplication Note 00-08-01\\nHigh Current Thermal Vias & Output Pins\\nThis shows that the [i2] term is independent of the pin and board temperatures, and the [i2R] heating term can now be calculated directly.\\nEquation 14\\nTemperature Rise Due to i2R Heating in the Spokes of a Thermal Via as a Function of Length\\n∆T = i2/(2*σ\\nr*w2*t2*(-k))*(x2-l*x)\\nWhere the maximum temperature increase will be at the center of the spoke [x = l/2]. Substituting [γ] for the aspect ratio of the spokes [γ=l/w],\\nEquation 15 Maximum Temperature Rise Due to i2R Heating in a Thermal Via Spoke\\n∆T = i2γ2/(8*σ\\nr*t2*k)\\nExample 1: Internal Heat Generation in a Typical Thermal Via\\nA typical thermally relieved via has 4 spokes with an aspect ratio [γ] of 0.6 (15 mil length, 25 mil trace width). If a circuit board has only one copper plane with a thickness of 2 ounces [t=66E-6 m] and 60 amps is driven through one thermal via (15 amps per spoke), Equation 16 predicts a temperature increase at l/2 of 0.12°C.\\n∆T(l/2) = 152*0.62/(8*50E6*(66E-6)2*386) = 0.12°C\\nExample 2: Temperature Drop Due to Internal Heat Generation in a Severe Thermal Via\\nConsider an extreme example where a thermally relieved via has four spokes of one ounce copper [t=33E-6 m], and the spokes are twice as long as they are wide [γ=2]. Using Equation 16, the temperature increase at the center of the spoke when 60 amps is driven through the pin (15 amps per spoke), is 5.36°C.\\n∆T(l/2) = 152*22/(8*50E6*(33E-6)2*386) = 5.36°C\\nIn this extreme case, a temperature rise of 5.36°C is expected at the center of the spoke due to i2R heating.\\nThe heat generation within a typical thermal via is not expected to create a significant temperature rise, even with 60 amps delivered through one via. However, the squared dependence of heat generation on both the aspect ratio and reciprocal of the thickness of the thermal spokes makes it possible to design a thermal via that does create a modest temperature increase.\\n4.0 Thermal Relief Designs and Power Loss - Observed To verify the above predictions an evaluation board was designed by SynQor such that the thermal character- istics of four different thermal reliefs could be observed. The thermal evaluation board (TEB) is documented as SynQor part number 031-1000026 Rev 1. The thermal reliefs vary in the length-to-width aspect ratio or scale of the spokes. The designed and measured geometry of the thermal relief spokes is listed in Table 2, see Figure 3 for the dimensioning key.\\nVia #\\nVia 1 Via 2 Via 3 Via 4\\nLength (designed/measured) 0.040/0.044 0.060/0.067 0.060/0.067 0.080/0.085\\nWidth (designed/measured) 0.060/0.067 0.060/0.067 0.090/0.095 0.060/0.065\\nAspect Ratio (length/width) 2:3 1:1 2:3 4:3\\nTable 2: Details of Thermal Relief Spokes\\nSynQor - Advancing the Power Curve (cid:127) 888-567-9596 (cid:127) www.synqor.com\\nPage 4\\nApplication Note 00-08-01\\nHigh Current Thermal Vias & Output Pins\\nLength\\nWidth\\nFigure 3: Thermal Relief Dimensioning Standard\\nTwo 2.5V, 60A half-brick PowerQor Tera converters, SynQor part# PQ48025HTA60NKS were each solder mounted to a TEB. The spokes of the output thermal reliefs were severed on the side of the TEB to which the board was mounted. This was done so that all current would flow through the observable spokes on the oppo- site side. The copper is specified to be one ounce, plated to two ounces. The spokes were monitored with a thermal imaging camera while the 2.5 volt power converters delivered 60 amps at 25°C. Airflow was 300 LFM, airflow direction was from the output pins towards the input pins. Still images were captured with an infrared camera after the power converter and TEB reached steady state temperature. Close up thermal images of each sample were recorded, and from those images a plot was made detailing the spoke temperature in °C as a function of distance from the center of the pin outwards measured in pixels. By analyzing the second deriv- ative of the temperature versus distance graphs, we are able to determine if heat is being generated within the spokes of the thermal relief. While the first derivative is an indication of the slope, the second derivative is an indicator of the \"curve\" within the line. Essentially a curved line shows self-heating within the spoke, i.e. the tem- perature increases as you go away from the pin.\\nImages 1 and 3, are two thermal reliefs with an aspect ratio of 2:3 (L:W). There is only negligible change between the two thermal images with a spoke aspect ratio of 2:3, which supports our derivation that only the thickness and aspect ratio of thermal relief spokes determine the thermal performance of a via. The image with a spoke aspect ratio of 1:1 (Image 2) is similar to those with an aspect ratio of 2:3. Plots 1, 2, and 3 show plotted data of Temperature versus Distance for the various thermal reliefs. As expected, each of these graphs show an almost linear temperature reduction as you move further out from the pin, indicating that there is no self heating within the spokes of the relief.\\nThere are two distinct differences between Image 4 and the others due to the 4:3 aspect ratio of the relief. The \"hub\" temperature is several degrees warmer, and the temperature within the thermal relief spokes is greatest at the mid-length of the spokes. The first observation indicates that the increased aspect ratio of the spokes pre- vents heat from conducting away from the center of the thermal relief. The second observation is evidence of noticeable i2R heating along the length of this geometry of spoke. Graphing the data from Image 4 yields the Plots 4 and 5. These graphs indicate strongly the self-heating of the poorly designed thermal via. The increase in temperature along the length of the spoke is on the order of 4 degrees, which is approximated with the fol- lowing equation:\\nEquation 16\\n∆Tmidlength = i2α2/(t2σ\\nκ) r\\nwhere i is current in amps, alpha is the spoke aspect ratio, t is copper thickness, σ r is the electrical conductivity of copper (50e6 A2/WM), and κ is the thermal conductivity of copper (386 W/mK). For the geometry with the long spokes,\\nSynQor - Advancing the Power Curve (cid:127) 888-567-9596 (cid:127) www.synqor.com\\nPage 5\\nApplication Note 00-08-01\\nHigh Current Thermal Vias & Output Pins\\n∆Tmidlength = 152(4/3)2/((7.1e-5)2(50e6)(386)) = 4.1 degrees,\\nas observed.\\nThermal Images and Temperature Gradients\\nImage 1: Thermal image of Via #1 with a length/width aspect ratio of 2:3\\nMin 82.6oC, Mean, 94.6oC, Max 110.2oC, Std.Dev. 9.2oC, Length 30.0\\nPlot 1: Temperature gradient of Line 1 from Via #1 with a\\nlength/width aspect ratio of 2:3\\nSynQor - Advancing the Power Curve (cid:127) 888-567-9596 (cid:127) www.synqor.com\\nPage 6\\nApplication Note 00-08-01\\nHigh Current Thermal Vias & Output Pins\\nImage 2: Thermal image of Via #2 with a length/width aspect ratio of 1:1\\nMin 83.0oC, Mean, 93.8oC, Max 109.8oC, Std.Dev. 8.4oC, Length 30.0\\nPlot 2: Temperature gradient of Line 2 from Via #2 with a\\nlength/width aspect ratio of 1:1\\nSynQor - Advancing the Power Curve (cid:127) 888-567-9596 (cid:127) www.synqor.com\\nPage 7\\nApplication Note 00-08-01\\nHigh Current Thermal Vias & Output Pins\\nImage 3: Thermal image of Via #3 with a length/width aspect ratio of 2:3\\nMin 79.3oC, Mean, 92.3oC, Max 109.4oC, Std.Dev. 8.9oC, Length 30.0\\nPlot 3: Temperature gradient of Line 3 from Via #3 with a\\nlength/width aspect ratio of 2:3\\nSynQor - Advancing the Power Curve (cid:127) 888-567-9596 (cid:127) www.synqor.com\\nPage 8\\nApplication Note 00-08-01\\nHigh Current Thermal Vias & Output Pins\\nImage 4: Thermal image of Via #4 with a length/width aspect ratio of 4:3\\nMin 91.2oC, Mean, 106.1oC, Max 119.4oC, Std.Dev. 8.9oC, Length 30.0\\nMin 80.6oC, Mean, 99.6oC, Max 120.0oC, Std.Dev. 11.3oC, Length 30.0\\nPlot 4: Temperature gradient of Line 4 from Via #4\\nPlot 5: Temperature gradient of Line 5 from Via #4\\nwith a length/width aspect ratio of 4:3\\nwith a length/width aspect ratio of 4:3\\n155 Swanson Rd., Boxboro, MA 01719 Phone: 978-849-0600 Toll Free: 888-567-9596 Fax: 978-849-0602 Web: www.synqor.com e-mail: sales@synqor.com\\nSynQor - Advancing the Power Curve (cid:127) 888-567-9596 (cid:127) www.synqor.com\\nPage 9'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_name, content = process_last_text_file(preview_length=100)\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "856ecb00-8ba0-4810-8824-369ac92095ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 1811_GitHub_wp_part1.txt to speech...\n",
      "Processing chunk 1 of 3...\n",
      "Audio stream saved successfully to audio_voiceovers/20240629_201258_1811_GitHub_wp_part1_part1_audio.mp3\n",
      "Audio file saved as: audio_voiceovers/20240629_201258_1811_GitHub_wp_part1_part1_audio.mp3\n",
      "Waiting for 60 seconds before processing the next chunk...\n",
      "Processing chunk 2 of 3...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m text_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdfs_to_convert_to_text\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_voiceovers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mconvert_txt_to_audio_voiceover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 339\u001b[0m, in \u001b[0;36mconvert_txt_to_audio_voiceover\u001b[0;34m(text_directory, output_directory)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_file \u001b[38;5;129;01min\u001b[39;00m text_files:\n\u001b[1;32m    338\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(text_directory, text_file)\n\u001b[0;32m--> 339\u001b[0m     \u001b[43mprocess_text_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay_between_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll text files have been processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 321\u001b[0m, in \u001b[0;36mprocess_text_file\u001b[0;34m(file_path, output_directory, delay_between_chunks)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_chunks):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 321\u001b[0m     audio_file_path \u001b[38;5;241m=\u001b[39m \u001b[43mtext_to_speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name_without_ext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m audio_file_path:\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio file saved as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 294\u001b[0m, in \u001b[0;36mtext_to_speech\u001b[0;34m(text, file_name, output_directory, chunk_index)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/condaenv311/lib/python3.11/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/condaenv311/lib/python3.11/site-packages/urllib3/response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 624\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/condaenv311/lib/python3.11/site-packages/urllib3/response.py:828\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    830\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/condaenv311/lib/python3.11/site-packages/urllib3/response.py:758\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[1;32m    759\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/condaenv311/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/condaenv311/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/condaenv311/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_directory = \"pdfs_to_convert_to_text\"\n",
    "output_directory = \"audio_voiceovers\"\n",
    "convert_txt_to_audio_voiceover(text_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9131ebc-7959-46ff-ac78-4f60b0920bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70417c-8f00-488a-9a56-d4cf9196fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173c91f-4a89-4624-a1f2-83cb683a5f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734d6fe-746f-43d1-b333-fe0071405466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd89527-3954-4403-a231-04bad5ee3c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1adee-8151-4032-b337-3d9d1f156691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25524379-3b5f-4af9-995d-c1547394e8b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for PDF URLs...\n",
      "Request parameters: {'q': 'neuroscience site:github.com filetype:pdf', 'key': 'AIzaSyDh6OU3OFcn_t3vrxO2fzGU49cT6pXpW0g', 'cx': 'f4afc285f53804afb', 'num': 5}\n",
      "Request URL: https://www.googleapis.com/customsearch/v1?q=neuroscience+site%3Agithub.com+filetype%3Apdf&key=AIzaSyDh6OU3OFcn_t3vrxO2fzGU49cT6pXpW0g&cx=f4afc285f53804afb&num=5\n",
      "Sending request to Google Custom Search API...\n",
      "Response received.\n",
      "Parsing response data...\n",
      "Response data parsed.\n",
      "Response data: {'kind': 'customsearch#search', 'url': {'type': 'application/json', 'template': 'https://www.googleapis.com/customsearch/v1?q={searchTerms}&num={count?}&start={startIndex?}&lr={language?}&safe={safe?}&cx={cx?}&sort={sort?}&filter={filter?}&gl={gl?}&cr={cr?}&googlehost={googleHost?}&c2coff={disableCnTwTranslation?}&hq={hq?}&hl={hl?}&siteSearch={siteSearch?}&siteSearchFilter={siteSearchFilter?}&exactTerms={exactTerms?}&excludeTerms={excludeTerms?}&linkSite={linkSite?}&orTerms={orTerms?}&dateRestrict={dateRestrict?}&lowRange={lowRange?}&highRange={highRange?}&searchType={searchType}&fileType={fileType?}&rights={rights?}&imgSize={imgSize?}&imgType={imgType?}&imgColorType={imgColorType?}&imgDominantColor={imgDominantColor?}&alt=json'}, 'queries': {'request': [{'title': 'Google Custom Search - neuroscience site:github.com filetype:pdf', 'totalResults': '1', 'searchTerms': 'neuroscience site:github.com filetype:pdf', 'count': 1, 'startIndex': 1, 'inputEncoding': 'utf8', 'outputEncoding': 'utf8', 'safe': 'off', 'cx': 'f4afc285f53804afb'}]}, 'context': {'title': 'github.com'}, 'searchInformation': {'searchTime': 0.238825, 'formattedSearchTime': '0.24', 'totalResults': '1', 'formattedTotalResults': '1'}, 'items': [{'kind': 'customsearch#result', 'title': 'A neuro-computational model of visual attention with multiple ...', 'htmlTitle': 'A <b>neuro</b>-computational model of visual attention with multiple ...', 'link': 'https://github.com/nilumbra/pinealand/wiki/attention/A%20neuro-computational%20model%20of%20visual%20attention%20with%20multiple%20attentional%20control%20sets.pdf', 'displayLink': 'github.com', 'snippet': 'Nov 5, 2021 ... In numerous activities, humans need to attend to multiple sources of visual information at the same time. Although several recent studies\\xa0...', 'htmlSnippet': 'Nov 5, 2021 <b>...</b> In numerous activities, humans need to attend to multiple sources of visual information at the same time. Although several recent studies&nbsp;...', 'formattedUrl': 'https://github.com/.../A%20neuro-computational%20model%20of%20visua...', 'htmlFormattedUrl': 'https://github.com/.../A%20neuro-computational%20model%20of%20visua...', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQj7JJHGEdzHLsyPTs62Ea2vgp57OEr--kLj7asQgZc4uZf_jnIvJXDD4VI&s', 'width': '215', 'height': '235'}], 'metatags': [{'creator': 'Elsevier', 'crossmarkdomains_1_': 'elsevier.com', 'creationdate': 'D:20211105210329Z', 'crossmarkdomains_2_': 'sciencedirect.com', 'author': 'Shabnam Novin', 'subject': 'Vision Research, 189 (2021) 104-118. doi:10.1016/j.visres.2021.08.009', 'creationdate__text': '5th November 2021', 'crossmarkmajorversiondate': '2010-04-23', 'elsevierwebpdfspecifications': '7.0', 'title': 'A neuro-computational model of visual attention with multiple attentional control sets', 'crossmarkdomainexclusive': 'true', 'moddate': 'D:20211105214835Z', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'doi': '10.1016/j.visres.2021.08.009'}], 'cse_image': [{'src': 'x-raw-image:///67f669745c0270a570f7a4cb46292d4cc94c5e23169b9a859c8f4b401a3918fb'}]}, 'mime': 'application/pdf', 'fileFormat': 'PDF/Adobe Acrobat'}]}\n",
      "Found 'items' in response data.\n",
      "Added URL: https://github.com/nilumbra/pinealand/wiki/attention/A%20neuro-computational%20model%20of%20visual%20attention%20with%20multiple%20attentional%20control%20sets.pdf\n",
      "Found 1 PDF URLs.\n",
      "Search completed.\n",
      "PDF URLs found:\n",
      "https://github.com/nilumbra/pinealand/wiki/attention/A%20neuro-computational%20model%20of%20visual%20attention%20with%20multiple%20attentional%20control%20sets.pdf\n",
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=131811907665-252blpd0att90p8qbeiifjf4cpsh20e4.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8081%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.file&state=1sPElwb3ve9DAO1Txk68JhnjjnEjl1&access_type=offline\n",
      "File A%20neuro-computational%20model%20of%20visual%20attention%20with%20multiple%20attentional%20control%20sets.pdf uploaded successfully.\n",
      "All PDFs have been uploaded to Google Drive.\n",
      "Processing PDFs...\n",
      "Processing A%20neuro-computational%20model%20of%20visual%20attention%20with%20multiple%20attentional%20control%20sets.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilysu/anaconda3/envs/condaenv311/lib/python3.11/site-packages/pdfminer/layout.py:405: RuntimeWarning: coroutine 'run.<locals>.run_server' was never awaited\n",
      "  LTComponent.__init__(self, (x0, y0, x1, y1))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed A%20neuro-computational%20model%20of%20visual%20attention%20with%20multiple%20attentional%20control%20sets.pdf and uploaded A%20neuro-computational%20model%20of%20visual%20attention%20with%20multiple%20attentional%20control%20sets.txt\n",
      "All PDFs have been processed and converted to text.\n",
      "Downloading A%20neuro-computational%20model%20of%20visual%20attention%20with%20multiple%20attentional%20control%20sets.pdf...\n",
      "Converting A%20neuro-computational%20model%20of%20visual%20attention%20with%20multiple%20attentional%20control%20sets.pdf to text...\n",
      "Processed A%20neuro-computational%20model%20of%20visual%20attention%20with%20multiple%20attentional%20control%20sets.pdf\n",
      "All specified PDFs have been downloaded and converted to text.\n",
      "Created variable: Synqor_thermal_relief_study_str\n",
      "Content preview: Thermal & Reliability Study on High Current Thermal Vias & Output Pins Application Note 00-08-01 Rev...\n",
      "Every third character of the preview from Synqor_thermal_relief_study_str:\n",
      "Trl lbi u  gCrthm a&uuPspitno -- v\n",
      "Converting 1811_GitHub_wp_part1.txt to speech...\n",
      "Processing chunk 1 of 3...\n",
      "Audio stream saved successfully to audio_voiceovers/20240629_203757_1811_GitHub_wp_part1_part1_audio.mp3\n",
      "Audio file saved as: audio_voiceovers/20240629_203757_1811_GitHub_wp_part1_part1_audio.mp3\n",
      "Waiting for 60 seconds before processing the next chunk...\n",
      "Processing chunk 2 of 3...\n",
      "Error in text-to-speech conversion: 401\n",
      "{\"detail\":{\"status\":\"quota_exceeded\",\"message\":\"This request exceeds your quota. You have 3135 characters remaining, while 4962 characters are required for this request.\",\"character_used\":41699,\"character_limit\":39872}}\n",
      "Failed to convert chunk 2 of pdfs_to_convert_to_text/1811_GitHub_wp_part1.txt to speech.\n",
      "Waiting for 60 seconds before processing the next chunk...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m num_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     49\u001b[0m num_files_to_process \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 50\u001b[0m variable_name, content \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_keywords_and_pdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_files_to_process\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[70], line 42\u001b[0m, in \u001b[0;36mprocess_keywords_and_pdfs\u001b[0;34m(keywords, num_urls, num_files_to_process)\u001b[0m\n\u001b[1;32m     40\u001b[0m text_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdfs_to_convert_to_text\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_voiceovers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mconvert_txt_to_audio_voiceover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m variable_name, content\n",
      "Cell \u001b[0;32mIn[54], line 350\u001b[0m, in \u001b[0;36mconvert_txt_to_audio_voiceover\u001b[0;34m(text_directory, output_directory)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_file \u001b[38;5;129;01min\u001b[39;00m text_files:\n\u001b[1;32m    349\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(text_directory, text_file)\n\u001b[0;32m--> 350\u001b[0m     mp3_path \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_text_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay_between_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     mp3_paths\u001b[38;5;241m.\u001b[39mappend(mp3_path)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll text files have been processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[54], line 330\u001b[0m, in \u001b[0;36mprocess_text_file\u001b[0;34m(file_path, output_directory, delay_between_chunks)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(text_chunks) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelay_between_chunks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds before processing the next chunk...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 330\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay_between_chunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "def process_keywords_and_pdfs(keywords, num_urls, num_files_to_process):\n",
    "    # Search for PDF URLs on GitHub\n",
    "    print(\"Searching for PDF URLs...\")\n",
    "    pdf_urls = search_pdfs_on_github(keywords, num_urls)\n",
    "    print(\"Search completed.\")\n",
    "    print(\"PDF URLs found:\")\n",
    "    for url in pdf_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Set up Google Drive authentication\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "    flow = InstalledAppFlow.from_client_secrets_file(\n",
    "        'credentials.json',\n",
    "        scopes=SCOPES,\n",
    "        redirect_uri='http://localhost:8081/'\n",
    "    )\n",
    "    creds = flow.run_local_server(port=8081)\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    # Create folders in Google Drive\n",
    "    main_folder_id = find_or_create_folder('getpdfs')\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    subfolder_id = create_folder(current_time, main_folder_id)\n",
    "\n",
    "    # Upload PDFs to Google Drive\n",
    "    upload_pdf_to_google_drive(drive_service, pdf_urls, subfolder_id, num_files_to_process)\n",
    "\n",
    "    # Process PDFs\n",
    "    local_directory = \"pdfs_to_convert_to_text\"\n",
    "    process_pdfs(drive_service, subfolder_id, local_directory, num_files_to_process)\n",
    "\n",
    "    # Process last text file\n",
    "    variable_name, content = process_last_text_file(preview_length=100)\n",
    "\n",
    "    # Convert text to audio voiceover\n",
    "    text_directory = \"pdfs_to_convert_to_text\"\n",
    "    output_directory = \"audio_voiceovers\"\n",
    "    convert_txt_to_audio_voiceover(text_directory, output_directory)\n",
    "\n",
    "    return variable_name, content\n",
    "\n",
    "# Example usage:\n",
    "keywords = \"neuroscience\"\n",
    "num_urls = 5\n",
    "num_files_to_process = 1\n",
    "variable_name, content = process_keywords_and_pdfs(keywords, num_urls, num_files_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876767b-ec2a-43f0-8abb-8165b9d39c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit app\n",
    "st.title(\"PDF to Audio Voiceover Converter\")\n",
    "\n",
    "# User inputs\n",
    "keywords = st.text_input(\"Enter keywords:\", \"observability\")\n",
    "num_urls = st.number_input(\"Number of URLs to process:\", min_value=1, value=5)\n",
    "num_files_to_process = st.number_input(\"Number of files to process:\", min_value=1, value=1)\n",
    "\n",
    "if st.button(\"Process\"):\n",
    "    with st.spinner(\"Processing...\"):\n",
    "        mp3_path = process_keywords_and_pdfs(keywords, num_urls, num_files_to_process)\n",
    "    \n",
    "    if mp3_path and os.path.exists(mp3_path):\n",
    "        st.success(\"Processing complete!\")\n",
    "        \n",
    "        # Display audio player\n",
    "        audio_file = open(mp3_path, 'rb')\n",
    "        audio_bytes = audio_file.read()\n",
    "        st.audio(audio_bytes, format='audio/mp3')\n",
    "        \n",
    "        # Provide download link\n",
    "        st.download_button(\n",
    "            label=\"Download MP3\",\n",
    "            data=audio_bytes,\n",
    "            file_name=\"voiceover.mp3\",\n",
    "            mime=\"audio/mp3\"\n",
    "        )\n",
    "    else:\n",
    "        st.error(\"Failed to generate audio file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89a92872-c207-4dc3-abbc-1fadd3b11902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install streamlit nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "040439ac-6938-4ff6-91f2-9dbade67b7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Streamlit app...\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import io\n",
    "\n",
    "# Import your other necessary functions here\n",
    "# from your_module import process_keywords_and_pdfs, convert_txt_to_audio_voiceover\n",
    "\n",
    "def main():\n",
    "    st.title(\"PDF to Audio Voiceover Converter\")\n",
    "\n",
    "    # User inputs\n",
    "    keywords = st.text_input(\"Enter keywords:\", \"observability\")\n",
    "    num_urls = st.number_input(\"Number of URLs to process:\", min_value=1, value=5)\n",
    "    num_files_to_process = st.number_input(\"Number of files to process:\", min_value=1, value=1)\n",
    "\n",
    "    if st.button(\"Process\"):\n",
    "        with st.spinner(\"Processing...\"):\n",
    "            mp3_path = process_keywords_and_pdfs(keywords, num_urls, num_files_to_process)\n",
    "        \n",
    "        if mp3_path and os.path.exists(mp3_path):\n",
    "            st.success(\"Processing complete!\")\n",
    "            \n",
    "            # Display audio player\n",
    "            audio_file = open(mp3_path, 'rb')\n",
    "            audio_bytes = audio_file.read()\n",
    "            st.audio(audio_bytes, format='audio/mp3')\n",
    "            \n",
    "            # Provide download link\n",
    "            st.download_button(\n",
    "                label=\"Download MP3\",\n",
    "                data=audio_bytes,\n",
    "                file_name=\"voiceover.mp3\",\n",
    "                mime=\"audio/mp3\"\n",
    "            )\n",
    "        else:\n",
    "            st.error(\"Failed to generate audio file.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "import webbrowser\n",
    "\n",
    "def run_streamlit():\n",
    "    # Start the Streamlit app in a separate process\n",
    "    process = subprocess.Popen([\"streamlit\", \"run\", \"streamlit_app.py\"], \n",
    "                               stdout=subprocess.PIPE, \n",
    "                               stderr=subprocess.PIPE, \n",
    "                               text=True)\n",
    "    \n",
    "    # Wait for the app to start and print output\n",
    "    for i in range(30):  # Wait for up to 30 seconds\n",
    "        output = process.stdout.readline()\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "        if \"You can now view your Streamlit app in your browser.\" in output:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Open the Streamlit app in a new browser tab\n",
    "    webbrowser.open_new_tab(\"http://localhost:8501\")\n",
    "    \n",
    "    return process\n",
    "\n",
    "# Run the Streamlit app\n",
    "print(\"Starting Streamlit app...\")\n",
    "streamlit_process = run_streamlit()\n",
    "\n",
    "# To stop the Streamlit app later, you can use:\n",
    "# streamlit_process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46b226-f990-4a12-8b32-307815e80de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
